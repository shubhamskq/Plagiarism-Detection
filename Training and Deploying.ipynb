{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'plagdata'\n",
    "prefix = 'plagiarism_detector_ourmodel'\n",
    "input_data = sagemaker_session.upload_data(\n",
    "    path=data_dir, bucket=bucket, key_prefix=prefix)\n",
    "print(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empchk = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empchk.append(obj.key)\n",
    "    print(obj.key)\n",
    "assert len(empchk) != 0, 'S3 bucket is empty.'\n",
    "print('Test passed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "estimator = SKLearn(entry_point='train.py',\n",
    "                    framework_version=\"0.20.0\",\n",
    "                    py_version=\"py3\",\n",
    "                    source_dir='srcsklearn',\n",
    "                    role=role,\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.c4.xlarge',\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    output_path=output_path,\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator.fit({'train': input_data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, instance_type='ml.t2.medium')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "datatst= pd.read_csv(os.path.join(\n",
    "    data_dir, \"test.csv\"), header=None, names=None)\n",
    "ytst = datatst.iloc[:, 0]\n",
    "xtst = datatst.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testypdct = predictor.predict(xtst)\n",
    "assert len(testypdct) == len(ytssrcsklearnd number of predictions'\n",
    "print('Test cases are passed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "nametarget = ['notplg', 'P']\n",
    "accuracy = accuracy_score(ytst, testypdct)\n",
    "print('Accuracy got:', accuracy)\n",
    "print(classification_report(ytst.values, testypdct, target_names=nametarget))\n",
    "print('\\nPredicted class labels are: ')\n",
    "print(testypdct)\n",
    "print('\\nTrue class labels are: ')\n",
    "print(ytst.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "delbucket = boto3.resource('s3').Bucket(bucket)\n",
    "delbucket.objects.all().delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictor, test_features, test_labels, verbose=True):\n",
    "    test_preds = np.squeeze(np.rint(predictor.predict(xtst))).astype(int)\n",
    "    tp = np.logical_and(test_labels, test_preds).sum()\n",
    "    fp = np.logical_and(1-test_labels, test_preds).sum()\n",
    "    tn = np.logical_and(1-test_labels, 1-test_preds).sum()\n",
    "    fn = np.logical_and(test_labels, 1-test_preds).sum()\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    if verbose:\n",
    "        print(pd.crosstab(test_labels, test_preds, rownames=[\n",
    "              'actuals'], colnames=['predictions']))\n",
    "        print(\"\\n{:<11} {:.3f}\".format('Recall of ourmodel got:', recall))\n",
    "        print(\"{:<11} {:.3f}\".format('Precision of ourmodel got:', precision))\n",
    "        print(\"{:<11} {:.3f}\".format('Accuracy of ourmodel got:', accuracy))\n",
    "        print()\n",
    "    return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn,\n",
    "            'Precision': precision, 'Recall': recall, 'Accuracy': accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "datatst= pd.read_csv(os.path.join(\n",
    "    data_dir, \"test.csv\"), header=None, names=None)\n",
    "ytst = datatst.iloc[:, 0]\n",
    "xtst = datatst.iloc[:, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testypdct = predictor.predict(xtst)\n",
    "assert len(testypdct) == len(ytst), 'Unexpected number of predictions got'\n",
    "print('Test cases are succesfully passed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "testypdct = np.squeeze(np.rint(predictor.predict(xtst))).astype(int)\n",
    "testypdct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nPredicted class labels are: ')\n",
    "print(testypdct)\n",
    "print('\\nTrue class labels are: ')\n",
    "print(ytst.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate(predictor, xtst, ytst.values, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enddel(predictor):\n",
    "    try:\n",
    "        boto3.client('sagemaker').enddel(\n",
    "            EndpointName=predictor.endpoint)\n",
    "        print('Deleted {}'.format(predictor.endpoint))\n",
    "    except:\n",
    "        print('Already deleted: {}'.format(predictor.endpoint))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enddel(predictor)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5ddf40e5bd61a8bc69543a3c08686f09c4123d2e0c41448ba5c17a1bd600af8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
